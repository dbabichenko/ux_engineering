{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Visualizing Qualitative VR Think-Aloud Data (HeritageRoots)\n",
    "# ---------------------------------------------------------------\n",
    "# Requirements:\n",
    "# pip install pandas nltk matplotlib seaborn wordcloud scikit-learn networkx\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 1. Load dataset ----------\n",
    "with open(\"heritageroots_ux_transcripts.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "participants = data[\"participants\"]\n",
    "\n",
    "# Flatten into DataFrame\n",
    "rows = []\n",
    "for p in participants:\n",
    "    for turn in p[\"transcript\"]:\n",
    "        if turn[\"speaker\"] == \"Participant\":\n",
    "            rows.append({\n",
    "                \"participant_id\": p[\"id\"],\n",
    "                \"bio\": p[\"bio\"],\n",
    "                \"time\": turn[\"time\"],\n",
    "                \"text\": turn[\"text\"]\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ---------- 2. Preprocess & sentiment scoring ----------\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df[\"compound\"] = df[\"text\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "df[\"label\"] = pd.cut(df[\"compound\"],\n",
    "                     bins=[-1, -0.05, 0.05, 1],\n",
    "                     labels=[\"negative\", \"neutral\", \"positive\"])\n",
    "\n",
    "# ---------- 3. WORD CLOUDS ----------\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def make_wordcloud(texts, title):\n",
    "    text = \" \".join(texts)\n",
    "    wc = WordCloud(width=900, height=500,\n",
    "                   background_color=\"white\",\n",
    "                   stopwords=stop_words,\n",
    "                   colormap=\"viridis\").generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "for label in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    subset = df[df[\"label\"] == label]\n",
    "    if not subset.empty:\n",
    "        make_wordcloud(subset[\"text\"], f\"Word Cloud â€“ {label.capitalize()} Sentiment\")\n",
    "\n",
    "# ---------- 4. THEME MAP (unsupervised clustering of frequent words) ----------\n",
    "# Build a term-document matrix\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_features=50)\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# PCA projection for visualization\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(X.toarray().T)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(coords[:,0], coords[:,1])\n",
    "for i, term in enumerate(terms):\n",
    "    plt.text(coords[i,0]+0.01, coords[i,1]+0.01, term, fontsize=9)\n",
    "plt.title(\"Theme Map (PCA of Term Frequencies)\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 5. SENTIMENT-OVER-TIME GRAPH ----------\n",
    "# Convert timestamps like [00:01:23] -> seconds\n",
    "def time_to_seconds(t):\n",
    "    try:\n",
    "        parts = t.strip(\"[]\").split(\":\")\n",
    "        return int(parts[0])*3600 + int(parts[1])*60 + int(parts[2])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df[\"seconds\"] = df[\"time\"].apply(time_to_seconds)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(data=df.sort_values(\"seconds\"), x=\"seconds\", y=\"compound\", hue=\"participant_id\", alpha=0.5, legend=False)\n",
    "sns.lineplot(x=df[\"seconds\"], y=df[\"compound\"], color=\"black\", linewidth=2, label=\"Overall trend\")\n",
    "plt.title(\"Sentiment Over Time Across All Participants\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Sentiment (compound score)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------- 6. CO-OCCURRENCE MATRIX ----------\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", max_features=20)\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "Xc = (X.T * X)  # co-occurrence matrix\n",
    "Xc.setdiag(0)\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "co_occurrence = pd.DataFrame(Xc.toarray(), index=terms, columns=terms)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(co_occurrence, cmap=\"YlGnBu\")\n",
    "plt.title(\"Word Co-Occurrence Matrix (Top 20 Terms)\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: visualize co-occurrence as network\n",
    "G = nx.Graph()\n",
    "for i, t1 in enumerate(terms):\n",
    "    for j, t2 in enumerate(terms):\n",
    "        if i < j and co_occurrence.iloc[i,j] > 0:\n",
    "            G.add_edge(t1, t2, weight=co_occurrence.iloc[i,j])\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "pos = nx.spring_layout(G, k=0.5)\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u,v in edges]\n",
    "nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=weights,\n",
    "        width=[w/2 for w in weights], edge_cmap=plt.cm.Blues, font_size=10)\n",
    "plt.title(\"Co-occurrence Network of Frequent Words\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
